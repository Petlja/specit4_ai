Градијентни спуст
=================

.. |open| image:: ../../_images/algk2.png
            :width: 100px

.. |gsf1| image:: ../../_images/gsf1.png
            :width: 180px

.. |gsf2| image:: ../../_images/gsf2.png
            :width: 180px

.. |gsf3| image:: ../../_images/gsf3.png
            :width: 180px

.. |gsf0| image:: ../../_images/gsf0.png
            :width: 180px

.. infonote::

 У овој лекцији упознаћемо градијентни спуст, технику која нам помаже да пронађемо параметре за које функција средњеквадратне грешке има најмању 
 вредност. Ову технику можемо, под одређеним условима, да применимо и на друге функције.

Опет ћеш морати нешто да замислиш - овога пута да се налазиш на врху једне лепе планине. То и не пада тако тешко! Невоља је што сада следи задатак: 
да се брзо спустиш до подножја! Један начин да то урадиш је да прво погледаш око себе и провериш дуж ког правца у твојој околини је планина 
најстрмија - имај на уму да треба баш брзо да се спустиш! Онда можеш да направиш један пажљиви корак у том правцу па да застанеш и опет погледаш 
око себе. Опет можеш да приметиш дуж ког правца је планина најсртрмија у твојој околини, направиш корак у том правцу и застанеш. Јасно ти је да 
овај редослед осматрања, избора правца и искорака можеш да наставиш да понављаш све док не стигнеш до подножја. Ту те чека освежење за успешно 
обављени задатак!  

.. figure:: ../../_images/gs1.png
    :width: 450
    :align: center

(слика је преузета са https://creazilla.com/nodes/37642-river-flowing-through-the-mountains-clipart)

-------

Мало физичке активности у сред приче о линеарној регресији није на одмет, али наслућујеш да постоји још нешто. Функција срењеквадратне грешке 
зависи од избора параметара ꞵ0 и ꞵ1 - за различите комбинације вредности ꞵ0 и ꞵ1 добијамо различите вредности грешке. Ако нацртамо график ове 
функције, на пример, дуж x-осе забележимо вредности ꞵ0, дуж y-осе забележимо вредности ꞵ1, а дуж z-осе вредности грешке, добићемо график који 
изгледа као на доњој слици. Ако црвеном тачком обележимо неки насумични избор параметара ꞵ0 и ꞵ1, да бисмо стигли до тачке за коју је вредност 
грешке најмања заиста морамо да се спустимо у подножје ове површи. Зато је ”техника” коју смо развили у претходном примеру врло релеванта. 
Потребно је само да осмислимо како да тражимо најстрмије правце спуста. У томе ће нам помоћи изводи функција. 

.. figure:: ../../_images/gs2.png
    :width: 450
    :align: center

*График функције средњеквадратне грешке*

**(ТОДО: прилагодити обележја а и б)**

-------

.. infonote::

 Најмања вредност једне функције се зове минимум. 

Посматрајмо сада квадратну функција f(x) = (x-1)2 чији је график приказан на доњој слици и покушајмо да техником спуста стигнемо до њеног минимума - 
он је у тачки x=1 и износи 0.  

.. image:: ../../_images/gs3.png
    :width: 500
    :align: center

Уочимо и црвену тачку која одговара вредности x=3 (насумично смо је одабрали) и која означава стартну позицију кретања ка минимуму ове функције. 
Делује да је наранџастом линијом обележен најстрмији правац дуж кога можемо да започнемо спуст. Занимљиво је да ова линија заправо представља 
тангенту наше функције у тачки x=3. Ако дуж овог правца направимо корак, наћи ћемо се у новој тачки. Обележимо и њену вредност црвеном бојом и 
прикажимо је на графику. Она је мало ближе очекиваном минимуму.

.. image:: ../../_images/gs4.png
    :width: 500
    :align: center

Сада можемо да поновимо поступак: нацртајмо тангенту у новој тачки, а потом и направимо корак дуж тог правца. 

.. image:: ../../_images/gs5.png
    :width: 500
    :align: center

Након одређеног броја корака, овај поступак ће нас довести до минимума функције тј. до тачке x=1.

.. image:: ../../_images/gs6.png
    :width: 500
    :align: center

У Google Coalb свесци која прати овај материјал можеш и сам да покренеш анимацију и увериш се да је тако. Довољно је да кликнеш на дугме |open| 
и отвориш је. 

Пре него што детаљније прођемо кроз поступак који смо описали, подсетимо се какве су то праве тангенте.  За неку фиксирану тачку x коефицијент 
правца тангенте у тачки x једнак је вредности првог извода извода функције у тачки x. Први извод наше функције је функција f’(x) = 2x-2 и у 
почетној тачки x=3 вредност извода је f’(x) = 4.  То значи да тангента има једначину y = 4x - 8 (број -8 смо добили из услова да ова права 
мора да садржи тачку (3, 4)).  Зато можемо и да кажемо да тангента има правац који одговара изводу функције у некој тачки, а за само кретање 
у том правцу да је кретање дуж правца извода у тој тачки.  Сада је дилема, да ли се крећемо уз или низ  тј. да ли пратимо правац извода или 
његов супротан правац? Па, пошто желимо да се спуштамо ка минимуму треба да пратимо правац супротан правцу извода функције. 

Ако сада са x0 oбележимо почетну тачку, нову тачку x1 добили смо тако што смо направили корак дуж правца извода функције у тачки x0. Ако са α 
обележимо дужину корака, вредност нове тачке  x1 израчунавамо као x1 = x0 - α f’(x0). Пошто поступак понављамо, вредност тачке x2  израчунавамо 
као x2 = x1 - α f’(x1) и настављамо редом са израчунавањима x3 = x2 - α f’(x2),  x4 = x3 - α f’(x3), …  Поступак понављамо све док за две 
узастопне вредности, рецимо за x34 и x35, вредности функције нису довољно близу тј. док апсолутна вредност разлике између f(x35) - f(x34) није мања од неке 
унапред задате течности, рецимо 0.001. Тако рачунски можемо да се приближимо појму конвергенције у математици. 

Вредност α коју смо увели се зове корак учења и представља врло важан параметар алгоритма који смо описали. Уколико су вредности за α много мале, 
требаће нам пуно времена да стигнемо до минимума. Са друге стране ако су вредности за α много велике, може се десити да прескочимо минимум или 
западнемо у цик-цак замку сталним какутањима око њега! Погледај доњу слику!

.. figure:: ../../_images/gs7.png
    :width: 780
    :align: center

*Утицај избора корака учења*

-------

.. figure:: ../../_images/gs8.png
    :width: 280
    :align: center

*Цик-цак замка*

-------

Оба ова понашања обавезно провери и сам у пратећој свесци користећи различита подешавања за корак учења у анимацији. 

Алгоритам који смо описали се зове **градијентни спуст** (енгл. *gradient descent*) и упркос својој једноставности представља један од најважнијих 
алгоритма у машинском учењу јер омогућава проналажење најмање вредности функције грешке. Постоји много детаља у вези са овим алгоритмом у које 
ми нећемо залазити а који се тичу особина функција на које овај алгоритам може успешно да се примени, нумеричког израчунавања извода и избора 
корака учења. Сви они се морају размотрити приликом практичне примене алгоритма. 

Сам алгоритам није неугодно испрограмирати па ћемо се упустити у авантуру. Потребна нам је функција ``f`` која ће да рачуна вредност задате функције и 
функција ``f_izvod`` која ће да израчунава вредност извода задате функције. Потребно је да дефинишемо и корак учења ``alfa`` и зауставне критеријуме: 
поступак ћемо обуставити када вредности функције у двема узастопним итерацијама буду довољно близу (разлика њихових вредности је мања од неке 
унапред задате тачности ``epsilon``) или када достигнемо неки коначан број итерација ``max_broj_iteracija`` (морамо да се осигурамо и у случајевима 
неподесних избора корака учења).   

Следи блок са кодом. Алгоритам смо започели постављањем почетне тачке. Како тачка у којој се померимо алгоритмом градијентног спуста представља 
почетну тачку наредног корака, за њихово обележавање у узастопним корацима користимо ознаке ``x_staro`` и ``x_novo``. Извештај који креирамо на крају 
функције садржи информације о томе да ли се алгоритам зауставио, колико му је корака тј. итерација требало и коју вредност је пронашао. 

.. image:: ../../_images/gs9.png
    :width: 600
    :align: center

Функцију коју смо разматрали и њен извод можемо дефинисати следећим Python блоковима: 

.. code-block:: Python

 def f(x):
    return (x-1)**2

 def f_izvod(x):
    return 2*x-2

Након покретања функције gradijentni_spust за вредности аргумената ``x0 = 3``, ``alfa = 0.1``, ``epsilon = 0.001`` и ``max_broj_iteracija = 100`` добијамо 
да је минимум функције број 1.0048 што можемо и да потврдимо. Код можеш и сам да извршиш и увериш се да се добија баш овај резулат. 
Не пропусти да испиташ и како се резултати мењају уколико се одаберу друге вредности аргумената.

Сада можемо да се вратимо и на проблем проналажења параметара ꞵ0 и ꞵ1 линеарне регресије за коју вредност средњеквадратне грешке треба да има 
најмању вредност. Функција средњеквадратне грешке је функција двеју променљивих - зависи и од вредности параметра ꞵ0 и од вредности параметра ꞵ1. 
Када радимо са функцијама више променљивих, у општем случају са n променљивих x1, x2, x3,...,xn , извод који смо користили у алгоритму градијентног 
спуста уопштавамо вектором извода - за сваку од променљивих израчунавамо појединачно изводе. Рецимо, за функцију |gsf0|, извод по променљивој x1 се 
добија тако што се променљива x2 прогласи константом па потом примене стандардна правила за рачунање извода којa нас доводе до ½*2*x1=x1. 
Са друге стране, извод по променљивој x2 се рачуна тако што се променљива x1 прогласи константом па примене стандардна правила за рачунање 
извода. Сада добијамо ½*10*2*x2=10x2. Сада добијамо да је вектор извода по појединачним променљивама (такве изводе зовемо парцијалним) 
вектор [x1, 10x2].  У математици, па и у машинском учењу, ови вектори се зову **градијенти** па отуда долази и име самог алгоритма. 
За обележавање градијената се користи симбол троуглић надоле ∇ који се зове набла. Тако би прецизан запис градијента полазне функције 
f био ∇f(x1,x2) = [x1, 10x2] и омогућавао би нам да пратимо дуж којих праваца извода појединачно треба да се крећемо приликом спуста. 

**TODO: сличица са правцима**

Остали кораци алгоритма градијентног спуста не разлику пуно за случај функција више променљивих. Како је сада решење које тражимо тачка у 
дводимензионом простору, критеријум заустављања можемо да прилагодимо тако што ћемо узети у обзир еуклидско растојање између вредности 
функција уместо њихове апсолутне разлике. 

Сада када разумемо и како градијентни спуст функционише за функције више променљивих вратимо се на израчунавање параметара ꞵ0 и ꞵ1. 
Рекли смо да је једначина средњеквадратне грешке |gsf1|. Пошто је то функција за коју треба да пронађемо минимум, ако засучемо рукаве па проверимо, 
добићемо да је извод средњеквадратне функције по  ꞵ0 баш |gsf2| и извод по ꞵ1 баш |gsf3|. Ови изводи нам указују дуж којих правада треба да се крећемо и 
колико треба да коригујемо вредности за ꞵ0 и ꞵ1 у сваком кораку итерације градијентног спуста.

У свесци можеш да видиш и како се ове вредности израчунавају кроз код, а потом и да прођеш кроз цео поступак прилагођеног градијентног спуста. 
За скуп о некретнинама који смо увели, добиће се вредности ꞵ0=2.056 и ꞵ1=1.198.

Рекли смо да постоје одређени предуслови које функција треба да задовољи да би њен минимум могао да се пронађе техником градијентног спуста 
(ако баш желиш да чујеш, потребно је да функција буде непрекидна и диференцијабилна). Важно је да знаш да се у општем случају на овај начин 
достиже неки локални минимум. Рецимо, функција на доњој слици има неколико локалних минимума и само један глобални минимум. У неким случајевима, 
рецимо када је функција конвексна, локални и глобални минимум се поклапају па увек стижемо до жељеног решења, глобалног минимума. Функција 
средњеквадратне грешке је конвексна по поараметрима ꞵ0 и ꞵ1.

.. figure:: ../../_images/gs10.png
    :width: 450
    :align: center

*Локални и глобални минимум*

(слика је преузета са https://vitalflux.com/local-global-maxima-minima-explained-examples/, **ТОДО: прилагодити обележја и натписе**)

-------

Област математике која се бави проналажењем максималних и минималних вредности функција (једним именом их зовемо оптимумима) зове се **математичка оптимизација**. 
Градијентни спуст је само један алгоритам из палете ове области. 
